---
title: "secchi_prep"
output: github_document

params: 
    datasource: csv
---
```{r setup}
## source common libraries, directories, functions, etc
source('~/github/bhi/baltic2015/prep/common.r')
dir_cw    = file.path(dir_prep, 'CW')
dir_secchi    = file.path(dir_prep, 'CW/secchi')

## add a README.md to the prep directory with the rawgit.com url for viewing on GitHub
create_readme(dir_secchi, 'secchi_prep.rmd')
```

## Background on using Secchi

[HELCOM Water Clarity Core Indicator](http://www.helcom.fi/baltic-sea-trends/indicators/water-clarity)  Mean Summer Secchi (June-September)  
**HELCOM Good Environmental Status**
"Good environmental status is measured in relation to scientifically based and commonly agreed sub-basin-wise target levels.  

These GES boundaries were based on the results obtained in the TARGREV project (HELCOM 2013a), taking also advantage of the work carried out during the EUTRO PRO project (HELCOM 2009) and national work for WFD. The final targets were set through an expert evaluation process done by the intersessional activity on development of core eutrophication indicators (HELCOM CORE EUTRO) and the targets were adopted by the HELCOM Heads of Delegations 39/2012."  

[Approaches and methods for eutrophication target setting in the Baltic Sea region](http://www.helcom.fi/Documents/Ministerial2013/Associated%20documents/Background/Eutorophication%20targets_BSEP133.pdf)

[Fleming-Lehtinen and Laamanen. 2012. Long-term changes in Secchi depth and the role of phytoplankton in explaining light attenuation in the Baltic Sea. Estuarine, Coastal, and Shelf Science 102-103:1-10](http://www.sciencedirect.com/science/article/pii/S0272771412000418)


## Secchi Data

### Data sources
**ICES**  
Data extracted from database and sent by Hjalte Parner.  
* "extraction from our database classified into HELCOM Assessment Units â€“ HELCOM sub basins with coastal WFD water bodies or water types"

**SMHI**  
Downloaded from [SMHI Shark database](http://www.smhi.se/klimatdata/oceanografi/havsmiljodata/marina-miljoovervakningsdata) on 23 February 2016 by Lena Viktorsson.  
* Download notes: datatyp: Physical and Chemical; Parameter: secchi depth  
Lena  did not exclude any data when she downloaded it.

## Data Cleaning and decision-making
**Duplicates in the Data**  
ICES data contains profile data (eg temperature,but secchi is only measured once). Need only unique secchi records.  It appears the SMHI also contains profiles. Also check to see if any SMHI data already in the ICES records.

**Coastal data**  
See decisions (map) Fleming-Lehtinen and Laamanen 2012. Need to decide if need to filter out the coastal sampling data.

**Sampling frequency**
*Can these data decisions be implemented?*
Fleming-Lehtinen and Laamanen (2012) do the following:  
1. If several observations were made on the same day in the vicinity of one another, they set max observation to 1 per day.  
2. If trips were made with objective to study seasonal algae blooms, a maximum of two observations were accepted to avoid bias.  
*We have not implemented this so far*

```{r data load}
## read in secchi data
data1 = readr::read_csv(file.path(dir_secchi, 'secchi_data_database/ices_secchi.csv'))
data2 = readr::read_csv(file.path(dir_secchi, 'secchi_data_database/smhi_secchi.csv'))

## Data overview
dim(data1)
colnames(data1)
str(data1)

dim(data2)
colnames(data2)
str(data2)

## Initial filtering
ices <- data1 %>% data.frame()%>% filter(!is.na(BHI_ID)) %>%
  select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
         lat= Latitude, lon = Longitude, 
         cruise= Cruise, station = Station, date= Date) %>%
  mutate(date = as.Date(date, format= "%Y-%m-%d"))%>%
  mutate(supplier = 'ices')
head(ices)


smhi <- data2 %>% data.frame()%>%
  filter(!is.na(BHI_ID)) %>%
  rename(secchi = value) %>%
  select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
        lat= Latitude, lon= Longitude, 
         cruise = Provtagningstillfaelle.id, 
         station = Stationsnamn, date= Date) %>%
  mutate(supplier = 'smhi', cruise = as.character(cruise))
head(smhi)

## Look for duplicate data

## is any data duplicated in ices itself
ices.duplicated = duplicated(ices)
sum(ices.duplicated==TRUE) #180841  ## MANY duplicates 

ices.duplicated = duplicated(select(ices,-station))
sum(ices.duplicated==TRUE) #180963  ## more duplicated when remove station columns
    ## it is not because of multiple cruises on same day and location
    ## tried by removing lat and lon and keeping station, fewer duplicates detected

## duplicates because ICES table includes deptp
new_ices = unique(select(ices,-station)); nrow(new_ices)  #take only unique records # 32896


## is any data duplicated in smhi itself
smhi.duplicated = duplicated(select(smhi, -station))
sum(smhi.duplicated==TRUE) #56938 ## MANY duplicates  ## removing station does not affect it
new_smhi = unique(select(smhi, -station)); nrow(new_smhi) #take only unique records # 10818

## use setdiff() to indentify data smhi not in ices
new_smhi = setdiff(select(new_smhi,-supplier,-cruise),select(new_ices,-supplier,-cruise)) %>%
            mutate(supplier = "smhi")
nrow(new_smhi) # 10357
## it appears 461 records are duplicates (if remove cruise and station)
## if date, lat, lon, secchi all match, I think they are duplicates

## Now create a new allData, bind only the new_smhi object to ices
allData = bind_rows(new_ices,new_smhi)
nrow(allData) # 43253
allData %>% select(year, month, date, cruise, lat, lon,secchi) %>% distinct() %>%nrow(.)  #43253

## what if remove cruise
allData %>% select(year, month, date, lat, lon,secchi) %>% distinct() %>%nrow(.)
# 43253 

```

## Target values
These are the values that will be used as a reference point.
```{r helcom target secchi}
target <- readr::read_csv(file.path(dir_cw, "eutro_targets_HELCOM.csv"))
head(target)

#select just summer_seccchi target
target = target %>% select(basin, summer_secchi)%>%
        mutate(basin = str_replace_all(basin,"_"," "))

```

## HELCOM HOLAS Basin
These basins are the relevant physical units.  
Secchi data will be first assessed at this level and then assigned to BHI region. EEZ divisions may result in some BHI regions that have no data but they are physically the same basin as a BHI region with data.

```{r basin lookup}
basin_lookup = readr::read_csv(file.path(
  dir_prep,"baltic_rgns_to_bhi_rgns_lookup_holas.csv"))
basin_lookup=basin_lookup %>% select(bhi_id = rgn_id, basin_name)%>%
  mutate(basin_name = str_replace_all(basin_name,"_"," "))

```


## Select summer data and plot
Months 6-9 (June, July, August, September)
Years >= 2000
Data is sparse for BHI regions 4,22,25
```{r select summer data}
summer = allData %>% filter(month %in%c(6:9)) %>%
        filter(year >=2000)
head(summer)

#Plot
ggplot(summer) + geom_point(aes(month,secchi, colour=supplier))+
  facet_wrap(~bhi_id, scales ="free_y")

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~bhi_id)


```

## Assign secchi data to a HOLAS basin
Data coverage appears substantially better at the basin scale.
Some basins have missing data or limited data for the most recent years: Great Belt, Gulf of Riga, Kiel Bay
```{r assign summer data to a HOLAS basin}

summer = summer %>% full_join(., basin_lookup, by="bhi_id")

#Plot
ggplot(summer) + geom_point(aes(month,secchi, colour=supplier))+
  facet_wrap(~basin_name, scales ="free_y")

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~basin_name, scales ="free_y")


```


## Restrict data to before 2014
There are still basins with limited or not data from 2010 onwards but this at least removes the potential for not having data reported in the past 2 years
```{r restrict data before 2014}
summer = summer %>% filter(year < 2014)

#Plot
ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~basin_name, scales ="free_y")


```

## Evaluate number of stations sampled in each basin
Very different number of unique lat-lon locations by month and basin.  
Sometimes lat-lon is not good to use because recording specific ship location which might be vary even though ship is at the same station. More duplicates were detected in the data however when station was not included, than when lat and lon were not included as the location identifier.  


```{r samples and stations by basin}

basin_summary = summer %>% group_by(basin_name,year,month)%>%
                select(year, month,lat,lon,basin_name)%>%
                summarise(loc_count = n_distinct(lat,lon))
basin_summary

#plot sampling overview
ggplot(basin_summary) + geom_point(aes(year,loc_count, colour=factor(month)))+
  facet_wrap(~basin_name, scales ="free_y")+
  ylab("Number Sampling Locations")



```


## Calculate mean monthly value for each summer month & overall mean
basin monthly mean = mean of all samples within month and basin
basin summer mean = mean of basin monthly mean values

```{r calculate summer secchi}

mean_months = summer %>% select(year, month,basin_name,secchi)%>%
              group_by(year,month,basin_name)%>%
              summarise(mean_secchi = round(mean(secchi,na.rm=TRUE),1))%>%
              ungroup()
head(mean_months)

#Plot
ggplot(mean_months) + geom_point(aes(year,mean_secchi, colour=factor(month)))+
  geom_line(aes(year,mean_secchi, colour=factor(month)))+
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

mean_months_summer = mean_months %>% select(year, basin_name,mean_secchi) %>%
                      group_by(year,basin_name)%>%
                      summarise(mean_secchi = round(mean(mean_secchi,na.rm=TRUE),1)) %>%
                      ungroup()  #in mean calculation all some months to have NA, ignore for that years calculation

ggplot(mean_months_summer) + geom_point(aes(year,mean_secchi))+
  geom_line(aes(year,mean_secchi))+
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

```


## Plot summer secchi with target values indicated
```{r summer secchi with target }

secchi_target = left_join(mean_months_summer,target, by=c("basin_name" = "basin"))%>%
                dplyr::rename(target_secchi = summer_secchi)
head(secchi_target)

ggplot(secchi_target) + geom_point(aes(year,mean_secchi))+
  geom_line(aes(year,target_secchi))+
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

```


## What year will the status be calculated for for each basin?
This is if there is no temporal gapfilling.  
Most basins can have the status calculated for 2013 with the exception of the Great Belt and Gulf of Riga.  
One option is no gap filling and calculating the status for differenet final years and over a different 5 year period
```{r what is the last year}
## get the last year of non-NA data
last_year = secchi_target%>%
            filter(!is.na(mean_secchi))%>%
            group_by(basin_name)%>%
            summarise(last_year = last(year))

##which are not in 2013
last_year %>% filter(last_year < 2013)

 
```

## Status calculation non-modeled data
Status must be calculated in data prep because calculation for a basin and then applying to all regions.  
*Status code based on code Lena Viktorsson developed for functions.r

```{r Status calculation}

## Define constants for status calculation

  min_year = 2000        # earliest year to use as a start for regr_length timeseries
                          ##data already filtered for 
  regr_length = 10       # number of years to use for regression
  future_year = 5        # the year at which we want the likely future status
  min_regr_length = 5    # min actual number of years with data to use for regression.

  
## Basin data with target
  secchi_target
  
  
## Calculate basin status
  ## Xnut = basin_mean/basin_target
  
  basin_status = secchi_target %>%
                 mutate(., status =  pmin(1, mean_secchi/target_secchi)) %>%
      select(basin_name, year, status)
  
## Calculate basin trend
  
  basin_trend =
    basin_status %>%
    group_by(basin_name) %>%
    do(tail(. , n = regr_length)) %>%  # calculate trend only if there is at least X years of data (min_regr_length) in the last Y years of time serie (regr_length)
    do({if(sum(!is.na(.$status)) >= min_regr_length)
    data.frame(trend_score = 
                 max(-1, min(1, coef(lm(status ~ year, .))['year'] * future_year)))
         else data.frame(trend_score = NA)}) %>%
    ungroup() 

  
  ## Assign basin status and trend to BHI regions
    bhi_status = basin_status %>%
                group_by(basin_name)%>%
                summarise_each(funs(last), basin_name, status)%>% #select last year of data for status in each basin (this means status year differs by basin)
                mutate(status = round(status*100))%>% #status is whole number 0-100
                ungroup()%>%
                left_join(basin_lookup,.,by="basin_name")%>% #join bhi regions to basins
                mutate(dimension = 'status') %>%
                select (rgn_id = bhi_id, dimension, score=status)
                
  
    bhi_trend = left_join(basin_lookup,basin_trend, by="basin_name") %>%
                 mutate(score = round(trend_score,2),
                        dimension = "trend")%>%
                select(rgn_id = bhi_id, dimension, score )
               


```

## Plot Basin status over time

```{r plot basin status}

ggplot(basin_status) + geom_point((aes(year,status)))+
  facet_wrap(~basin_name)

```

## Plot BHI region status and trend values
Status values can range from 0-100  
Trend values can be between -1 to 1
```{r bhi status and trend plot}

ggplot(full_join(bhi_status,bhi_trend, by=c("rgn_id","dimension","score"))) + geom_point(aes(rgn_id,score),size=2)+
  facet_wrap(~dimension, scales="free_y")+
  xlab("BHI region")


```


## Save csv files
These csv files will be used as a first cut for the secchi status and trend.
**Files to save**  
1. Status and trend for each BHI region based on the basin level calculations
```{r write csv file to layers}
## Write csv files to layers
    readr::write_csv(bhi_status, 
                 file.path(dir_layers, "cw_nu_status_bhi2015.csv"))
    
    readr::write_csv(bhi_trend, 
                 file.path(dir_layers, "cw_nu_trend_bhi2015.csv"))

```


## Next steps
**Mean secchi calculation**  
1. Find out how mean secchi value determined for HELCOM core indicator - is our calculation of the mean ok?  *Thorsten contacting Vivi Fleming-Lehtinen*  

**Trend calculation**  
1. Have calculated the trend using data spanning 10 years (minimum of 5 data points). Is it agreed that we should use the longer time window for the trend?  

**Should/How to model the data?**  
Not all months are sampled in all years (see above plot).  
If model, do a linear model.  
*Linear model options*
1. Take mean summer secchi, ignore that different months sampled in different years, just average the months that are sampled in any given year.  Model by basin and year.  
2. Take mean monthly value by year, model by basin + year + month. Average the modelled monthly value to get a summer mean.  
3. Do the above but just model all the data points, don't take the mean value and instead use a random effect to account for location?  
